{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ccec73a-ff0b-4315-aca3-d3bd08bc7bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python3 code for preprocessing text \n",
    "import nltk \n",
    "import re \n",
    "import numpy as np \n",
    "\n",
    "# execute the text here as : \n",
    "# text = \"\"\" # place text here \"\"\" \n",
    "text = \"\"\"\n",
    "Beans. I was trying to explain to somebody as we were flying in, that’s corn. That’s beans. And they were very impressed at my agricultural knowledge. Please give it up for Amaury once again for that outstanding introduction. I have a bunch of good friends here today, including somebody who I served with, who is one of the finest senators in the country, and we’re lucky to have him, your Senator, Dick Durbin is here. I also noticed, by the way, former Governor Edgar here, who I haven’t seen in a long time, and somehow he has not aged and I have. And it’s great to see you, Governor. I want to thank President Killeen and everybody at the U of I System for making it possible for me to be here today. And I am deeply honored at the Paul Douglas Award that is being given to me. He is somebody who set the path for so much outstanding public service here in Illinois. Now, I want to start by addressing the elephant in the room. I know people are still wondering why I didn’t speak at the commencement.\n",
    "\"\"\"\n",
    "dataset = nltk.sent_tokenize(text) \n",
    "for i in range(len(dataset)): \n",
    "\tdataset[i] = dataset[i].lower() \n",
    "\tdataset[i] = re.sub(r'\\W', ' ', dataset[i]) \n",
    "\tdataset[i] = re.sub(r'\\s+', ' ', dataset[i]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6363c72-351d-4328-b1d2-3cd102163f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' beans ', 'i was trying to explain to somebody as we were flying in that s corn ', 'that s beans ', 'and they were very impressed at my agricultural knowledge ', 'please give it up for amaury once again for that outstanding introduction ', 'i have a bunch of good friends here today including somebody who i served with who is one of the finest senators in the country and we re lucky to have him your senator dick durbin is here ', 'i also noticed by the way former governor edgar here who i haven t seen in a long time and somehow he has not aged and i have ', 'and it s great to see you governor ', 'i want to thank president killeen and everybody at the u of i system for making it possible for me to be here today ', 'and i am deeply honored at the paul douglas award that is being given to me ', 'he is somebody who set the path for so much outstanding public service here in illinois ', 'now i want to start by addressing the elephant in the room ', 'i know people are still wondering why i didn t speak at the commencement ']\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d147e1c-12e2-40a9-a274-6959970bef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model \n",
    "word2count = {} \n",
    "for data in dataset: \n",
    "\twords = nltk.word_tokenize(data) \n",
    "\tfor word in words: \n",
    "\t\t\tword2count[word] = 1+word2count.get(word,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8ee6f3c-6812-4ccc-a6f0-fb7c7d47b15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'beans': 2, 'i': 12, 'was': 1, 'trying': 1, 'to': 8, 'explain': 1, 'somebody': 3, 'as': 1, 'we': 2, 'were': 2, 'flying': 1, 'in': 5, 'that': 4, 's': 3, 'corn': 1, 'and': 7, 'they': 1, 'very': 1, 'impressed': 1, 'at': 4, 'my': 1, 'agricultural': 1, 'knowledge': 1, 'please': 1, 'give': 1, 'it': 3, 'up': 1, 'for': 5, 'amaury': 1, 'once': 1, 'again': 1, 'outstanding': 2, 'introduction': 1, 'have': 3, 'a': 2, 'bunch': 1, 'of': 3, 'good': 1, 'friends': 1, 'here': 5, 'today': 2, 'including': 1, 'who': 4, 'served': 1, 'with': 1, 'is': 4, 'one': 1, 'the': 9, 'finest': 1, 'senators': 1, 'country': 1, 're': 1, 'lucky': 1, 'him': 1, 'your': 1, 'senator': 1, 'dick': 1, 'durbin': 1, 'also': 1, 'noticed': 1, 'by': 2, 'way': 1, 'former': 1, 'governor': 2, 'edgar': 1, 'haven': 1, 't': 2, 'seen': 1, 'long': 1, 'time': 1, 'somehow': 1, 'he': 2, 'has': 1, 'not': 1, 'aged': 1, 'great': 1, 'see': 1, 'you': 1, 'want': 2, 'thank': 1, 'president': 1, 'killeen': 1, 'everybody': 1, 'u': 1, 'system': 1, 'making': 1, 'possible': 1, 'me': 2, 'be': 1, 'am': 1, 'deeply': 1, 'honored': 1, 'paul': 1, 'douglas': 1, 'award': 1, 'being': 1, 'given': 1, 'set': 1, 'path': 1, 'so': 1, 'much': 1, 'public': 1, 'service': 1, 'illinois': 1, 'now': 1, 'start': 1, 'addressing': 1, 'elephant': 1, 'room': 1, 'know': 1, 'people': 1, 'are': 1, 'still': 1, 'wondering': 1, 'why': 1, 'didn': 1, 'speak': 1, 'commencement': 1}\n"
     ]
    }
   ],
   "source": [
    "print(word2count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "291ff91c-96d6-4e0e-a7a1-66495fa756ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'the', 'to', 'and', 'in', 'for', 'here', 'that', 'at', 'who', 'is', 'somebody', 's', 'it', 'have', 'of', 'beans', 'we', 'were', 'outstanding', 'a', 'today', 'by', 'governor', 't', 'he', 'want', 'me', 'was', 'trying', 'explain', 'as', 'flying', 'corn', 'they', 'very', 'impressed', 'my', 'agricultural', 'knowledge', 'please', 'give', 'up', 'amaury', 'once', 'again', 'introduction', 'bunch', 'good', 'friends', 'including', 'served', 'with', 'one', 'finest', 'senators', 'country', 're', 'lucky', 'him', 'your', 'senator', 'dick', 'durbin', 'also', 'noticed', 'way', 'former', 'edgar', 'haven', 'seen', 'long', 'time', 'somehow', 'has', 'not', 'aged', 'great', 'see', 'you', 'thank', 'president', 'killeen', 'everybody', 'u', 'system', 'making', 'possible', 'be', 'am', 'deeply', 'honored', 'paul', 'douglas', 'award', 'being', 'given', 'set', 'path', 'so']\n"
     ]
    }
   ],
   "source": [
    "import heapq \n",
    "freq_words = heapq.nlargest(100, word2count, key=word2count.get)\n",
    "print(freq_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e417a10b-fc1a-4bd3-a37c-4b554c5a87a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " beans \n",
      "i was trying to explain to somebody as we were flying in that s corn \n",
      "that s beans \n",
      "and they were very impressed at my agricultural knowledge \n",
      "please give it up for amaury once again for that outstanding introduction \n",
      "i have a bunch of good friends here today including somebody who i served with who is one of the finest senators in the country and we re lucky to have him your senator dick durbin is here \n",
      "i also noticed by the way former governor edgar here who i haven t seen in a long time and somehow he has not aged and i have \n",
      "and it s great to see you governor \n",
      "i want to thank president killeen and everybody at the u of i system for making it possible for me to be here today \n",
      "and i am deeply honored at the paul douglas award that is being given to me \n",
      "he is somebody who set the path for so much outstanding public service here in illinois \n",
      "now i want to start by addressing the elephant in the room \n",
      "i know people are still wondering why i didn t speak at the commencement \n"
     ]
    }
   ],
   "source": [
    "for data in dataset:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "843aa2bd-2be5-4449-9ccf-2ca56530a52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['i' '0']\n",
      "  ['the' '0']\n",
      "  ['to' '0']\n",
      "  ...\n",
      "  ['set' '0']\n",
      "  ['path' '0']\n",
      "  ['so' '0']]\n",
      "\n",
      " [['i' '1']\n",
      "  ['the' '0']\n",
      "  ['to' '1']\n",
      "  ...\n",
      "  ['set' '0']\n",
      "  ['path' '0']\n",
      "  ['so' '0']]\n",
      "\n",
      " [['i' '0']\n",
      "  ['the' '0']\n",
      "  ['to' '0']\n",
      "  ...\n",
      "  ['set' '0']\n",
      "  ['path' '0']\n",
      "  ['so' '0']]\n",
      "\n",
      " ...\n",
      "\n",
      " [['i' '0']\n",
      "  ['the' '1']\n",
      "  ['to' '0']\n",
      "  ...\n",
      "  ['set' '1']\n",
      "  ['path' '1']\n",
      "  ['so' '1']]\n",
      "\n",
      " [['i' '1']\n",
      "  ['the' '1']\n",
      "  ['to' '1']\n",
      "  ...\n",
      "  ['set' '0']\n",
      "  ['path' '0']\n",
      "  ['so' '0']]\n",
      "\n",
      " [['i' '1']\n",
      "  ['the' '1']\n",
      "  ['to' '0']\n",
      "  ...\n",
      "  ['set' '0']\n",
      "  ['path' '0']\n",
      "  ['so' '0']]]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "for data in dataset:\n",
    "    vector = []\n",
    "    for word in freq_words:\n",
    "        if word in nltk.word_tokenize(data):\n",
    "            vector.append([word,1])\n",
    "        else:\n",
    "            vector.append([word,0])\n",
    "    X.append(vector)\n",
    "X = np.asarray(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5285a303-76c3-4c6b-b496-82eaa59b8bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fox jumped table\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = \"The fox jumped on the table\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "filtered_words = [token.text for token in doc if not token.is_stop]\n",
    "clean_text = \" \".join(filtered_words)\n",
    "\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98f32843-9961-494f-80d0-7be3834eac1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\madhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "382a89bf-adca-4015-9bab-2a4a1bad01fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "len(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f55e6fe0-e666-45d0-9251-ea92b731bd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "on\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = \"The fox jumped on the table\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    if token.is_stop:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4719ad16-e47f-4c37-9bb7-dc2f2dbaee19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thor swimppersimz': 4,\n",
       " 'swimppersimz is': 3,\n",
       " 'is looking': 1,\n",
       " 'looking for': 2,\n",
       " 'for sandwitch': 0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "v = CountVectorizer(ngram_range = (2,2))\n",
    "v.fit([\"Thor Swimppersimz is looking for a sandwitch\"])\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61f8f11a-8af0-4886-b2ec-99e107e2caa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Thor ate pizza\",\n",
    "    \"Loki is tall\",\n",
    "    \"Loki is eating pizza\"\n",
    "]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "882844f2-4f3c-4ae8-a825-12c034d751a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thor eat pizza'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    filtered_tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        else:\n",
    "            filtered_tokens.append(token.lemma_) #Base word\n",
    "\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "preprocess(\"Thor ate a pizza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb145eab-17f6-4af7-b6ae-fa7b633fa7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thor eat pizza', 'Loki tall', 'Loki eat pizza']\n"
     ]
    }
   ],
   "source": [
    "corpus_processed = [preprocess(text) for text in corpus]\n",
    "print(corpus_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a024143-8bb6-489d-bc3f-4075750d4e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thor': 7,\n",
       " 'eat': 0,\n",
       " 'pizza': 5,\n",
       " 'thor eat': 8,\n",
       " 'eat pizza': 1,\n",
       " 'loki': 2,\n",
       " 'tall': 6,\n",
       " 'loki tall': 4,\n",
       " 'loki eat': 3}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = CountVectorizer(ngram_range = (1,2))\n",
    "v.fit(corpus_processed)\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b68f2d3-a2a9-4512-8082-dc2728dc4294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 1, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.transform([\"Thor eat pizza\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b81875b5-ad38-4678-b010-142caa2f0bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"She sells seashells by the seashore.\",\n",
    "    \"Peter Piper picked a peck of pickled peppers.\",\n",
    "    \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\",\n",
    "    \"Sally sells sea shells down by the seashore.\",\n",
    "    \"The sun sets in the west.\",\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"Alice was beginning to get very tired of sitting by her sister on the bank.\",\n",
    "    \"In a hole in the ground there lived a hobbit.\",\n",
    "    \"It was the best of times, it was the worst of times.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c4cba87-2153-4a6c-a938-e0877f8ec0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 48, 'quick': 35, 'brown': 4, 'fox': 11, 'jumps': 21, 'over': 28, 'lazy': 22, 'dog': 9, 'she': 43, 'sells': 41, 'seashells': 39, 'by': 5, 'seashore': 40, 'peter': 31, 'piper': 34, 'picked': 32, 'peck': 29, 'of': 26, 'pickled': 33, 'peppers': 30, 'how': 17, 'much': 25, 'wood': 56, 'would': 59, 'woodchuck': 57, 'chuck': 7, 'if': 18, 'could': 8, 'sally': 36, 'sea': 38, 'shells': 44, 'down': 10, 'sun': 47, 'sets': 42, 'in': 19, 'west': 55, 'cat': 6, 'sat': 37, 'on': 27, 'mat': 24, 'alice': 0, 'was': 54, 'beginning': 2, 'to': 52, 'get': 12, 'very': 53, 'tired': 51, 'sitting': 46, 'her': 14, 'sister': 45, 'bank': 1, 'hole': 16, 'ground': 13, 'there': 49, 'lived': 23, 'hobbit': 15, 'it': 20, 'best': 3, 'times': 50, 'worst': 58}\n"
     ]
    }
   ],
   "source": [
    "v = TfidfVectorizer()\n",
    "transformed_output = v.fit_transform(corpus)\n",
    "print(v.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4468d535-bf8b-456e-944e-73445f6aa1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice : 2.7047480922384253\n",
      "bank : 2.7047480922384253\n",
      "beginning : 2.7047480922384253\n",
      "best : 2.7047480922384253\n",
      "brown : 2.7047480922384253\n",
      "by : 2.01160091167848\n",
      "cat : 2.7047480922384253\n",
      "chuck : 2.7047480922384253\n",
      "could : 2.7047480922384253\n",
      "dog : 2.7047480922384253\n",
      "down : 2.7047480922384253\n",
      "fox : 2.7047480922384253\n",
      "get : 2.7047480922384253\n",
      "ground : 2.7047480922384253\n",
      "her : 2.7047480922384253\n",
      "hobbit : 2.7047480922384253\n",
      "hole : 2.7047480922384253\n",
      "how : 2.7047480922384253\n",
      "if : 2.7047480922384253\n",
      "in : 2.2992829841302607\n",
      "it : 2.7047480922384253\n",
      "jumps : 2.7047480922384253\n",
      "lazy : 2.7047480922384253\n",
      "lived : 2.7047480922384253\n",
      "mat : 2.7047480922384253\n",
      "much : 2.7047480922384253\n",
      "of : 2.01160091167848\n",
      "on : 2.2992829841302607\n",
      "over : 2.7047480922384253\n",
      "peck : 2.7047480922384253\n",
      "peppers : 2.7047480922384253\n",
      "peter : 2.7047480922384253\n",
      "picked : 2.7047480922384253\n",
      "pickled : 2.7047480922384253\n",
      "piper : 2.7047480922384253\n",
      "quick : 2.7047480922384253\n",
      "sally : 2.7047480922384253\n",
      "sat : 2.7047480922384253\n",
      "sea : 2.7047480922384253\n",
      "seashells : 2.7047480922384253\n",
      "seashore : 2.2992829841302607\n",
      "sells : 2.2992829841302607\n",
      "sets : 2.7047480922384253\n",
      "she : 2.7047480922384253\n",
      "shells : 2.7047480922384253\n",
      "sister : 2.7047480922384253\n",
      "sitting : 2.7047480922384253\n",
      "sun : 2.7047480922384253\n",
      "the : 1.2006706954621513\n",
      "there : 2.7047480922384253\n",
      "times : 2.7047480922384253\n",
      "tired : 2.7047480922384253\n",
      "to : 2.7047480922384253\n",
      "very : 2.7047480922384253\n",
      "was : 2.2992829841302607\n",
      "west : 2.7047480922384253\n",
      "wood : 2.7047480922384253\n",
      "woodchuck : 2.7047480922384253\n",
      "worst : 2.7047480922384253\n",
      "would : 2.7047480922384253\n"
     ]
    }
   ],
   "source": [
    "all_feature_names = v.get_feature_names_out()\n",
    "\n",
    "for word in all_feature_names:\n",
    "    indx = v.vocabulary_.get(word)\n",
    "    print(f\"{word} : {v.idf_[indx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4e34e7e-7e1f-48c5-881e-696563f87c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick brown fox jumps over the lazy dog.',\n",
       " 'She sells seashells by the seashore.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "41bbd1f0-1df0-492a-a46e-8bc9c9e04b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.35832784,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.35832784,\n",
       "        0.        , 0.35832784, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.35832784, 0.35832784, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.35832784, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.35832784, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31813221, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.3630973 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.48821152,\n",
       "        0.41502439, 0.41502439, 0.        , 0.48821152, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.21672305, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_output.toarray()[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07976923-efc8-40f9-9837-b2951e077b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
